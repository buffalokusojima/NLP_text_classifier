{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分ち書きするコード\n",
    "\n",
    "## 使用するライブラリインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "import sys, os, glob\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import MeCab\n",
    "\n",
    "mecab = MeCab.Tagger('mecabrc')\n",
    "\n",
    "import string\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 関数定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多分使わない関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(txt) :\n",
    "    ##txt = txt.replace(\"\\n\",' ')\n",
    "  \n",
    "\n",
    "  ##txt = txt.replace(\"\\t\",' ')\n",
    "    ##txt = txt.replace(\"\\r\",' ')\n",
    "    for i in string.printable:       # 英数字・半角記号を取り除く\n",
    "        txt = txt.replace(i,' ',)\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分ち書きする関数\n",
    "#### テキストをインプットとして名詞と動詞を抽出してスペースで繋げた文字列を返す\n",
    "\n",
    "#### (例)航はオーストラリアへ行った => 航　オーストラリア　行く"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分かち書き\n",
    "def separate_words(text):\n",
    "    vocab_list_mecab = []\n",
    "    sents = mecab.parse(text)\n",
    "    sents = sents.split(\"\\n\") # 改行記号で解析結果を分割    \n",
    "    sents.remove('')\n",
    "    sents.remove('EOS')\n",
    "    tmp = ''\n",
    "    \n",
    "    #解析結果ごとに品詞を見て抽出するものを判断\n",
    "    for i in sents:\n",
    "        node1 = i.split(\"\\t\")\n",
    "        node2 = node1[1].split(\",\")\n",
    "        if (node2[0] == '名詞') & (node2[1] != '固有名詞') :\n",
    "            tmp = tmp+' '+node1[0]\n",
    "        elif node2[0] == '動詞' :\n",
    "            tmp = tmp+' '+node2[6]\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XMLファイルから文章取り出す関数\n",
    "#### 上手くデータを元に改良して欲しい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XMLファイルから文章を取り出す\n",
    "def get_XMLtext(text):\n",
    "    data = []\n",
    "    tree = ET.parse(j)\n",
    "    page = tree.getroot()\n",
    "    num = len(page.getchildren())    # ルートに紐付く子供のタグ数を取り出す\n",
    "    for k in np.arange(num) :        # 順番に処理して目当ての部分であれば、dataとして取り出す。\n",
    "        tmp = page.getchildren()[k].findtext('text')\n",
    "        if tmp !=None :\n",
    "            #tmp = text_cleaning(tmp)\n",
    "            data.append(tmp)\n",
    "    return \"\\n\".join(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不要な文章を削除する関数\n",
    "#### いい感じに不要だと思う文字とかパターンを追加して欲しい\n",
    "#### その際、不要なパターンや文字毎に関数を分けて定義した方が管理しやすい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不要な文章の削除\n",
    "def removeWords(text):\n",
    "    text = text.replace('\\u3000', '')\n",
    "    text = text.split(\"\\n\")\n",
    "    removeHead(text)\n",
    "    \n",
    "    #無駄な改行削除\n",
    "    text = [x for x in text if x]\n",
    "    \n",
    "    \"\"\"\n",
    "    ここに不要だと思う文章パターンを削除する関数を追記していく\n",
    "    \"\"\"\n",
    "    return \"\\n\".join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ヘッダ削除する関数\n",
    "#### get_XMLで補完できそうならこれはいらない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ヘッダ削除\n",
    "def removeHead(text):\n",
    "    del text[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分ち書きしたファイルを統合する関数\n",
    "#### 指定したフォルダにある.wakatiファイル全て結合する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ファイル結合\n",
    "def file_join():\n",
    "    for path in glob.glob(ROOT_DIR+DATA_PATH):\n",
    "        docs = []\n",
    "        if not os.path.isdir(path):\n",
    "            continue\n",
    "        print(path)\n",
    "        path_wakati = path+\"/total.wakati\"\n",
    "        for file in glob.glob(path+OUTPUT_FILE_NAME):\n",
    "            docs.append(open(file,\"r\", encoding='utf-8').read())\n",
    "        wt=\"\\n\".join(docs)\n",
    "        open(path_wakati, \"w\", encoding=\"utf-8\").write(wt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分ち書きファイルを作成する関数\n",
    "#### 対象のファイルがある同じディレクトリに.wakatiと拡張子が変わった同名ファイルを作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ファイル作成\n",
    "def make_files():\n",
    "    for path in glob.glob(ROOT_DIR + DATA_PATH + FILE_NAME, recursive=True):\n",
    "    \n",
    "        #分ち書き変換するファイルの拡張子を設定\n",
    "        path_wakati=path.replace(\".\"+FILE_NAME.split(\".\")[-1], \".wakati\")\n",
    "        #print(path_wakati)\n",
    "        #if os.path.exists(path_wakati): continue #ファイルができているときはスルー\n",
    "        text=open(path,\"r\", encoding='utf-8').read() #エンコーディングに注意\n",
    "    \n",
    "        #文章がXML形式であればXMLからTEXTを抽出する\n",
    "        if FILE_NAME.split(\".\")[-1] == \"xml\":\n",
    "            text = get_XMLtext(text)\n",
    "    \n",
    "        #URL的な要素削除\n",
    "        text=re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-…]+', \"\", text)\n",
    "\n",
    "        #不要な文章の削除\n",
    "        words=removeWords(text)\n",
    "    \n",
    "        #文章を句点毎に分ける\n",
    "        words = words.replace(\"。\", \"\\n\")\n",
    "    \n",
    "        #文章毎に分かち書きする\n",
    "        text = []\n",
    "        for word in words.split(\"\\n\"):\n",
    "            tmp = separate_words(word)\n",
    "            #tmpが空欄の場合もあるのでそれは排除\n",
    "            if tmp != '':\n",
    "                text.append(tmp)\n",
    "        #スペースつながりで分かち書きした言葉を繋げていく\n",
    "        wt=\"\\n\".join(text)\n",
    "        open(path_wakati, \"w\", encoding=\"utf-8\").write(wt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メイン関数\n",
    "#### 引数説明\n",
    "    ・ \"-m\" ・・・　ファイル作成関数呼び出し\n",
    "    ・ \"-a\" ・・・  ファイル結合関数呼び出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not correct number of the argument\n",
      "not correct argument\n"
     ]
    }
   ],
   "source": [
    "#ROOT_DIR = 'C:\\\\Users\\\\9047247\\\\Documents\\\\AI_contest_2019\\\\学習用データ'\n",
    "ROOT_DIR = '.'\n",
    "DATA_PATH = '/text/*'\n",
    "FILE_NAME = '/*.txt'\n",
    "OUTPUT_FILE_NAME = '/*.wakati'\n",
    "\n",
    "argv = sys.argv\n",
    "args = len(argv)\n",
    "if args != 2:\n",
    "    print(\"not correct number of the argument\")\n",
    "    exit(1)\n",
    "\n",
    "if argv[1] == '-m':\n",
    "    make_files()\n",
    "    exit(0)\n",
    "elif argv[1] == '-a':\n",
    "    file_join()\n",
    "    exit(0)\n",
    "else:\n",
    "    print(\"not correct argument\")\n",
    "    exit(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
